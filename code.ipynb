{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model #2"
      ],
      "metadata": {
        "id": "NNj-jfVpusn5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W-beEIVuqlV"
      },
      "outputs": [],
      "source": [
        "!pip install catboost imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTUAeSMuqlW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import itertools\n",
        "\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C1OVF47uqlW"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = \".\"\n",
        "DATA_DIR = os.path.join(ROOT_DIR, \".\")\n",
        "\n",
        "train_data_v1 =  pd.read_csv(os.path.join(DATA_DIR, \"data/train.csv\"))\n",
        "test_data_v1 = pd.read_csv(os.path.join(DATA_DIR, \"data/test.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPLviQuQuqlW"
      },
      "outputs": [],
      "source": [
        "\n",
        "lst2 = ['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam',\n",
        "       'CURE END POSITION X Collect Result_Dam',\n",
        "       'CURE SPEED Collect Result_Dam',\n",
        "       'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage1) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD Standby Position X Collect Result_Dam',\n",
        "       'Production Qty Collect Result_Dam', 'Receip No Collect Result_Dam',\n",
        "       'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
        "       'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam',\n",
        "       '1st Pressure Collect Result_AutoClave',\n",
        "       '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
        "       '2nd Pressure Collect Result_AutoClave',\n",
        "       '3rd Pressure Collect Result_AutoClave',\n",
        "       '3rd Pressure Unit Time_AutoClave',\n",
        "       'Chamber Temp. Collect Result_AutoClave',\n",
        "       'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1',\n",
        "       'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage1) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage3) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
        "       'Head Purge Position Z Collect Result_Fill1', 'Equipment_Fill2',\n",
        "       'CURE END POSITION X Collect Result_Fill2',\n",
        "       'CURE END POSITION Z Collect Result_Fill2',\n",
        "       'CURE SPEED Collect Result_Fill2',\n",
        "       'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
        "       'WorkMode Collect Result_Fill2',\n",
        "        'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2djMxa7cuqlX"
      },
      "outputs": [],
      "source": [
        "# Equipment_xxx 값 #1, #2로 통일\n",
        "train_data_v1['Equipment_Dam'] = train_data_v1['Equipment_Dam'].replace({\n",
        "    'Dam dispenser #1': '#1',\n",
        "    'Dam dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "train_data_v1['Equipment_Fill1'] = train_data_v1['Equipment_Fill1'].replace({\n",
        "    'Fill1 dispenser #1': '#1',\n",
        "    'Fill1 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "train_data_v1['Equipment_Fill2'] = train_data_v1['Equipment_Fill2'].replace({\n",
        "    'Fill2 dispenser #1': '#1',\n",
        "    'Fill2 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "# Equipment_xxx 값 #1, #2로 통일\n",
        "test_data_v1 ['Equipment_Dam'] = test_data_v1 ['Equipment_Dam'].replace({\n",
        "    'Dam dispenser #1': '#1',\n",
        "    'Dam dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data_v1 ['Equipment_Fill1'] = test_data_v1 ['Equipment_Fill1'].replace({\n",
        "    'Fill1 dispenser #1': '#1',\n",
        "    'Fill1 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data_v1 ['Equipment_Fill2'] = test_data_v1 ['Equipment_Fill2'].replace({\n",
        "    'Fill2 dispenser #1': '#1',\n",
        "    'Fill2 dispenser #2': '#2'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ_HmIQ9uqlX"
      },
      "outputs": [],
      "source": [
        "outlier_ok = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "              'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2']\n",
        "train_data_v1[outlier_ok] = train_data_v1[outlier_ok].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "test_data_v1[outlier_ok] = test_data_v1[outlier_ok].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "\n",
        "train_data_v1['target'] = train_data_v1['target'].apply(lambda x: 0 if x == 'Normal' else 1)\n",
        "\n",
        "train_data = train_data_v1[lst2]\n",
        "test_data = test_data_v1[lst2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLcRH3H0uqlY"
      },
      "outputs": [],
      "source": [
        "category=['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam', 'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_vOXU7OuqlY",
        "outputId": "87969ede-97f3-40fb-8613-6462ff9b6e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({0: 19078, 1: 19078})\n",
            "target\n",
            "0    19078\n",
            "1    19078\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X = train_data.drop('target', axis=1)\n",
        "y = train_data['target']\n",
        "\n",
        "# 범주형 변수의 인덱스 목록 생성\n",
        "categorical_features = [X.columns.get_loc(col) for col in category]\n",
        "\n",
        "# SMOTENC로 소수 클래스의 데이터를 다수 클래스의 절반 정도로 증강\n",
        "smote_nc = SMOTENC(categorical_features=categorical_features, sampling_strategy=0.5, random_state=42)\n",
        "\n",
        "# Undersampling을 통해 다수 클래스의 데이터를 소수 클래스와 동일하게 줄이기\n",
        "undersample = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
        "\n",
        "# 파이프라인을 통해 SMOTENC와 Undersampling 결합\n",
        "pipeline = Pipeline(steps=[('smote', smote_nc), ('undersample', undersample)])\n",
        "\n",
        "# 데이터에 파이프라인 적용하여 Resampling 수행\n",
        "X_res, y_res = pipeline.fit_resample(X, y)\n",
        "\n",
        "# Resampling 후 클래스 분포 확인\n",
        "print(f'Resampled dataset shape {Counter(y_res)}')\n",
        "\n",
        "# 결과 확인\n",
        "df_resampled = pd.concat([pd.DataFrame(X_res, columns=X.columns), pd.DataFrame(y_res, columns=['target'])], axis=1)\n",
        "# print(df_resampled)\n",
        "print(df_resampled['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-_M4fUAuqlY",
        "outputId": "a34c4e3c-7ad8-4f7f-f30d-495212f8167d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "bestTest = 0.9357418643\n",
            "bestIteration = 897\n",
            "\n",
            "Shrink model to first 898 iterations.\n",
            "\n",
            "bestTest = 0.9342159702\n",
            "bestIteration = 794\n",
            "\n",
            "Shrink model to first 795 iterations.\n",
            "\n",
            "bestTest = 0.9374827871\n",
            "bestIteration = 849\n",
            "\n",
            "Shrink model to first 850 iterations.\n",
            "\n",
            "bestTest = 0.9379632171\n",
            "bestIteration = 949\n",
            "\n",
            "Shrink model to first 950 iterations.\n",
            "\n",
            "bestTest = 0.9388145315\n",
            "bestIteration = 989\n",
            "\n",
            "Shrink model to first 990 iterations.\n"
          ]
        }
      ],
      "source": [
        "X = df_resampled.drop('target', axis=1)\n",
        "y = df_resampled['target']\n",
        "X_test = test_data.drop('target', axis=1)\n",
        "\n",
        "# Stratified K-Fold Cross-Validation 설정\n",
        "n_splits = 5  # K 값 설정\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "train_fold_predict_c = np.zeros((X.shape[0], 1))\n",
        "val_predict_c = np.zeros((X_test.shape[0], 5))\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "# K-Fold Cross-Validation을 통한 모델 학습 및 평가\n",
        "for cnt,(train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    x_train, x_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.1,\n",
        "        depth=6,\n",
        "        eval_metric='F1',\n",
        "        random_seed=42,\n",
        "        logging_level='Verbose',\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x_train, y_train,\n",
        "        cat_features=categorical_features,\n",
        "        eval_set=(x_val, y_val),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # 예측 및 평가\n",
        "    y_pred = model.predict(x_val)\n",
        "    train_fold_predict_c[val_index, : ] = y_pred.reshape(-1, 1)\n",
        "    val_predict_c[:, cnt] = model.predict(X_test)\n",
        "val_predict_mean_c = np.mean(val_predict_c, axis=1).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nIppBajuqlY"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "X = df_resampled.drop('target', axis=1)\n",
        "y = df_resampled['target']\n",
        "X_test = test_data.drop('target', axis=1)\n",
        "for col in category:\n",
        "  X[col] = X[col].astype('category').cat.codes\n",
        "  X_test[col] = X_test[col].astype('category').cat.codes\n",
        "# Stratified K-Fold Cross-Validation 설정\n",
        "n_splits = 5  # K 값 설정\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "train_fold_predict_l = np.zeros((X.shape[0], 1))\n",
        "val_predict_l = np.zeros((X_test.shape[0], 5))\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "# K-Fold Cross-Validation을 통한 모델 학습 및 평가\n",
        "for cnt,(train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "    x_train, x_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    model = LGBMClassifier(n_estimators=1000, num_leaves=64,learning_rate = 0.1, n_jobs=-1,verbose=-1, boost_from_average=False)\n",
        "\n",
        "    model.fit(\n",
        "        x_train, y_train,\n",
        "        eval_set=(x_val, y_val),\n",
        "        eval_metric = 'f1',\n",
        "    )\n",
        "\n",
        "    # 예측 및 평가\n",
        "    y_pred = model.predict(x_val)\n",
        "    train_fold_predict_l[val_index, : ] = y_pred.reshape(-1, 1)\n",
        "    val_predict_l[:, cnt] = model.predict(X_test)\n",
        "val_predict_mean_l = np.mean(val_predict_l, axis=1).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGMaHVBbuqlZ",
        "outputId": "58ed5171-8dde-48d1-a9f6-af07c57ad0f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(38156, 2) (17361, 2)\n"
          ]
        }
      ],
      "source": [
        "new_train = np.concatenate([train_fold_predict_l, train_fold_predict_c], axis=1)\n",
        "new_test = np.concatenate([val_predict_mean_l, val_predict_mean_c], axis=1)\n",
        "print(new_train.shape,new_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMJIUTP7uqlZ"
      },
      "outputs": [],
      "source": [
        "new_model =   LGBMClassifier(random_state = 42,n_estimators=1000, num_leaves=64,learning_rate = 0.1, n_jobs=-1,verbose=-1, boost_from_average=False)\n",
        "new_model.fit(new_train, y)\n",
        "pred = new_model.predict(new_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbNjfaU8uqlZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "new_model = RandomForestClassifier(random_state=42, n_estimators=1000)\n",
        "new_model.fit(new_train, y)\n",
        "pred = new_model.predict(new_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoFNGE0cuqlZ"
      },
      "outputs": [],
      "source": [
        "def apply_rules(df):\n",
        "    receip_columns = [col for col in df.columns if 'Receip No Collect Result' in col]\n",
        "    equipment_columns = [col for col in df.columns if 'Equipment' in col and col != 'Equipment_AutoClave']\n",
        "    qty_columns = [col for col in df.columns if 'Production Qty Collect Result' in col]\n",
        "\n",
        "    condition = (\n",
        "        (df[receip_columns].nunique(axis=1) > 1) |\n",
        "        (df[equipment_columns].nunique(axis=1) > 1) |\n",
        "        (df[qty_columns].nunique(axis=1) > 1) |\n",
        "        (df['Workorder_Dam'].isin([\"3KPXX094-0001\", \"4CPXX084-0001\"]))\n",
        "    )\n",
        "\n",
        "    return condition\n",
        "condition = apply_rules(test_data_v1)\n",
        "pred[condition]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5w89UOzuqlZ"
      },
      "outputs": [],
      "source": [
        "def get_clf_eval(y_test, y_pred=None):\n",
        "    #y_pred = np.array([0 if i==\"Normal\" else 1 for i in y_pred])\n",
        "    #y_test = np.array([0 if i==\"Normal\" else 1 for i in y_test])\n",
        "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
        "\n",
        "    print(\"오차행렬:\\n\", confusion)\n",
        "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
        "    print(\"정밀도: {:.4f}\".format(precision))\n",
        "    print(\"재현율: {:.4f}\".format(recall))\n",
        "    print(\"F1: {:.4f}\".format(F1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mtkhjahuqlZ",
        "outputId": "ce6c6707-5733-45e0-c775-7260ce01e295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "pred = np.where(pred == 1, 'AbNormal', 'Normal')\n",
        "\n",
        "df_sub = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))\n",
        "df_sub[\"target\"] = pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(os.path.join('./csv', \"model2_submission.csv\"), index=False)\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model #5\n"
      ],
      "metadata": {
        "id": "VFGYzhXfu4ea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yGczy4suqlZ"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Tuple, List\n",
        "import os\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from collections import Counter, defaultdict\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler, BorderlineSMOTE, ADASYN, KMeansSMOTE, SVMSMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8hjmXmAuqlZ"
      },
      "outputs": [],
      "source": [
        "def apply_encoding(X: pd.DataFrame, category: List[str], encoding_type: str) -> pd.DataFrame:\n",
        "    if encoding_type == \"onehot\":\n",
        "        # One-Hot Encoding 적용\n",
        "        encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "        X_encoded = encoder.fit_transform(X[category])\n",
        "        X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(category))\n",
        "        X = pd.concat([X.drop(columns=category).reset_index(drop=True), X_encoded_df.reset_index(drop=True)], axis=1)\n",
        "    elif encoding_type == \"label\":\n",
        "        # Label Encoding 적용\n",
        "        X = X.copy()\n",
        "        for col in category:\n",
        "            le = LabelEncoder()\n",
        "            X[col] = le.fit_transform(X[col])\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5L7k8i4uqlZ"
      },
      "outputs": [],
      "source": [
        "# 상수 설정\n",
        "DATA_DIR = './data'\n",
        "\n",
        "# 데이터 로드 및 주요 특성 선택\n",
        "train_data = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
        "test_data = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-HlkXeSuqla"
      },
      "outputs": [],
      "source": [
        "selected_features = [\n",
        "    'Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam',\n",
        "    'CURE END POSITION X Collect Result_Dam', 'CURE SPEED Collect Result_Dam',\n",
        "    'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
        "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
        "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
        "    'Dispense Volume(Stage1) Collect Result_Dam',\n",
        "    'Dispense Volume(Stage2) Collect Result_Dam',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
        "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
        "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
        "    'HEAD Standby Position X Collect Result_Dam',\n",
        "    'Production Qty Collect Result_Dam', 'Receip No Collect Result_Dam',\n",
        "    'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
        "    'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam',\n",
        "    '1st Pressure Collect Result_AutoClave',\n",
        "    '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
        "    '2nd Pressure Collect Result_AutoClave', '3rd Pressure Collect Result_AutoClave',\n",
        "    '3rd Pressure Unit Time_AutoClave', 'Chamber Temp. Collect Result_AutoClave',\n",
        "    'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1',\n",
        "    'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
        "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
        "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
        "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
        "    'Dispense Volume(Stage3) Collect Result_Fill1',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
        "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
        "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
        "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
        "    'Head Purge Position Z Collect Result_Fill1', 'Equipment_Fill2',\n",
        "    'CURE END POSITION X Collect Result_Fill2',\n",
        "    'CURE END POSITION Z Collect Result_Fill2', 'CURE SPEED Collect Result_Fill2',\n",
        "    'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
        "    'WorkMode Collect Result_Fill2',\n",
        "    'target'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zn9-vVeuqla"
      },
      "outputs": [],
      "source": [
        "train_data_v1 = train_data[selected_features]\n",
        "test_data_v1 = test_data[selected_features]\n",
        "\n",
        "# 타겟 변수 이진화 (Normal: 0, AbNormal: 1)\n",
        "train_data_v1.loc[:, 'target'] = (train_data_v1['target'] != 'Normal').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwEVt6PYuqla",
        "outputId": "148f9384-4d0d-4880-c7f7-ddff273db7d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_909/4176305379.py:2: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  columns_to_replace = [col for col in train_data_v1 if 'OK' in train_data_v1[col].unique() and col != 'Chamber Temp. Judge Value_AutoClave']\n"
          ]
        }
      ],
      "source": [
        "# 'OK' 값을 가진 열의 처리를 위한 컬럼 선택 및 값 변환\n",
        "columns_to_replace = [col for col in train_data_v1 if 'OK' in train_data_v1[col].unique() and col != 'Chamber Temp. Judge Value_AutoClave']\n",
        "train_data_v1.loc[:, columns_to_replace] = train_data_v1[columns_to_replace].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "test_data_v1.loc[:, columns_to_replace] = test_data_v1[columns_to_replace].apply(pd.to_numeric, errors='coerce').fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF7MQdpyuqla"
      },
      "outputs": [],
      "source": [
        "# Equipment_xxx 값 #1, #2로 통일\n",
        "train_data_v1.loc[:, 'Equipment_Dam'] = train_data_v1['Equipment_Dam'].replace({\n",
        "    'Dam dispenser #1': '#1',\n",
        "    'Dam dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "train_data_v1.loc[:, 'Equipment_Fill1'] = train_data_v1['Equipment_Fill1'].replace({\n",
        "    'Fill1 dispenser #1': '#1',\n",
        "    'Fill1 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "train_data_v1.loc[:, 'Equipment_Fill2'] = train_data_v1['Equipment_Fill2'].replace({\n",
        "    'Fill2 dispenser #1': '#1',\n",
        "    'Fill2 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "# Equipment_xxx 값 #1, #2로 통일\n",
        "test_data_v1.loc[:, 'Equipment_Dam'] = test_data_v1.loc[:, 'Equipment_Dam'].replace({\n",
        "    'Dam dispenser #1': '#1',\n",
        "    'Dam dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data_v1.loc[:, 'Equipment_Fill1'] = test_data_v1.loc[:, 'Equipment_Fill1'].replace({\n",
        "    'Fill1 dispenser #1': '#1',\n",
        "    'Fill1 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data_v1.loc[:, 'Equipment_Fill2'] = test_data_v1.loc[:, 'Equipment_Fill2'].replace({\n",
        "    'Fill2 dispenser #1': '#1',\n",
        "    'Fill2 dispenser #2': '#2'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e2CjDGwuqla"
      },
      "outputs": [],
      "source": [
        "# 모든 열에서 값을 숫자로 변환 시도\n",
        "train_data_v1 = train_data_v1.apply(pd.to_numeric, errors='ignore')\n",
        "test_data_v1 = test_data_v1.apply(pd.to_numeric, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy3boWdSuqla",
        "outputId": "4bf2962c-61a6-473f-e3aa-6d18f8d8d2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam', 'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2']\n"
          ]
        }
      ],
      "source": [
        "# 모든 열에서 숫자로 변환이 불가능한 열(column) 찾기\n",
        "non_numeric_columns = train_data_v1.columns[train_data_v1.apply(lambda col: pd.to_numeric(col, errors='coerce').notna().all()) == False].tolist()\n",
        "print(non_numeric_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRtEVMMVuqla"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Tuple, List\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from collections import Counter, defaultdict\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler, BorderlineSMOTE, ADASYN, KMeansSMOTE, SVMSMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUs3hRJWuqla"
      },
      "outputs": [],
      "source": [
        "def apply_encoding(X: pd.DataFrame, category: List[str], encoding_type: str) -> pd.DataFrame:\n",
        "    if encoding_type == \"onehot\":\n",
        "        # One-Hot Encoding 적용\n",
        "        encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "        X_encoded = encoder.fit_transform(X[category])\n",
        "        X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(category))\n",
        "        X = pd.concat([X.drop(columns=category).reset_index(drop=True), X_encoded_df.reset_index(drop=True)], axis=1)\n",
        "    elif encoding_type == \"label\":\n",
        "        # Label Encoding 적용\n",
        "        X = X.copy()\n",
        "        for col in category:\n",
        "            le = LabelEncoder()\n",
        "            X[col] = le.fit_transform(X[col])\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIs75lZpuqla",
        "outputId": "8cf96c62-164e-4b3a-e605-78c9efbc10b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resampled dataset shape: Counter({0: 14330, 1: 14330})\n",
            "target\n",
            "0    14330\n",
            "1    14330\n",
            "Name: count, dtype: int64\n",
            "\n",
            "bestTest = 0.955198162\n",
            "bestIteration = 974\n",
            "\n",
            "Shrink model to first 975 iterations.\n",
            "Fold F1 Score: 0.9551981619758759\n",
            "\n",
            "bestTest = 0.943839791\n",
            "bestIteration = 946\n",
            "\n",
            "Shrink model to first 947 iterations.\n",
            "Fold F1 Score: 0.9438397910317806\n",
            "\n",
            "bestTest = 0.9447425671\n",
            "bestIteration = 840\n",
            "\n",
            "Shrink model to first 841 iterations.\n",
            "Fold F1 Score: 0.9447425670775924\n",
            "\n",
            "bestTest = 0.9392924734\n",
            "bestIteration = 798\n",
            "\n",
            "Shrink model to first 799 iterations.\n",
            "Fold F1 Score: 0.9392924734313582\n",
            "Mean F1 Score: 0.9457682483791519\n"
          ]
        }
      ],
      "source": [
        "# 범주형 변수 선택\n",
        "category = train_data_v1.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# SMOTEENN과 RandomUnderSampler 설정\n",
        "oversampler = SMOTEENN(random_state=42, sampling_strategy=0.5)\n",
        "undersampler = RandomUnderSampler(random_state=42, sampling_strategy=1.0)\n",
        "\n",
        "# 데이터와 타겟 분리\n",
        "X = train_data_v1.drop('target', axis=1)\n",
        "y = train_data_v1['target']\n",
        "\n",
        "# One-Hot Encoding 적용\n",
        "X_encoded = apply_encoding(X, category, \"onehot\")\n",
        "\n",
        "# 파이프라인 설정 및 Resampling\n",
        "pipeline = Pipeline(steps=[('oversampler', oversampler), ('undersampler', undersampler)])\n",
        "X_res, y_res = pipeline.fit_resample(X_encoded, y)\n",
        "\n",
        "# Resampling 후 데이터프레임 생성\n",
        "df_resampled = pd.concat([pd.DataFrame(X_res, columns=X_encoded.columns), pd.DataFrame(y_res, columns=['target'])], axis=1)\n",
        "print(f\"Resampled dataset shape: {Counter(y_res)}\")\n",
        "print(df_resampled['target'].value_counts())\n",
        "\n",
        "# Resampling된 데이터를 다시 분리\n",
        "X_resampled = df_resampled.drop('target', axis=1)\n",
        "y_resampled = df_resampled['target']\n",
        "\n",
        "# Stratified K-Fold Cross-Validation 설정\n",
        "n_splits = 4\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "# K-Fold Cross-Validation을 통한 모델 학습 및 평가\n",
        "for train_index, val_index in skf.split(X_resampled, y_resampled):\n",
        "    x_train, x_val = X_resampled.iloc[train_index], X_resampled.iloc[val_index]\n",
        "    y_train, y_val = y_resampled.iloc[train_index], y_resampled.iloc[val_index]\n",
        "\n",
        "    # CatBoostClassifier 설정 및 학습\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.1,\n",
        "        depth=6,\n",
        "        eval_metric='F1',\n",
        "        random_seed=42,\n",
        "        logging_level='Verbose',\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x_train, y_train,\n",
        "        cat_features=None,  # 원핫인코딩 적용으로 cat_features 설정 불필요\n",
        "        eval_set=(x_val, y_val),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # 예측 및 평가\n",
        "    y_pred = model.predict(x_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    f1_scores.append(f1)\n",
        "    print(f\"Fold F1 Score: {f1}\")\n",
        "\n",
        "# 전체 F1 Score 평균\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "print(f\"Mean F1 Score: {mean_f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akmh1zDtuqla"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzkWZM27uqlb"
      },
      "outputs": [],
      "source": [
        "# 테스트 데이터 전처리\n",
        "X_test = test_data_v1.drop('target', axis=1, errors='ignore')\n",
        "\n",
        "# train_data_v1에서 사용한 것과 동일한 방식으로 One-Hot Encoding 적용\n",
        "X_test_encoded = apply_encoding(X_test, category, \"onehot\")\n",
        "\n",
        "# 학습된 모델을 사용하여 예측 수행\n",
        "y_test_pred = model.predict(X_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkSyBPU7uqlb"
      },
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = y_test_pred\n",
        "\n",
        "df_sub.loc[:, \"target\"] = np.where(df_sub['target'] == 1, \"AbNormal\", \"Normal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utKM_8iDuqlb"
      },
      "outputs": [],
      "source": [
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"./csv/model5_submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model #1"
      ],
      "metadata": {
        "id": "dTiwHsOrv1Q4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_NdoUEBuqlb",
        "outputId": "6a67fbaa-a1d6-4666-97ed-1f194e01cee6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lyu/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import itertools\n",
        "\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDmj62D6uqlb"
      },
      "source": [
        "### 엑셀 파일들 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gpF4HBquqlc",
        "outputId": "e9a2720b-6d2b-41b6-cbda-ca709399d87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40506, 464)\n"
          ]
        }
      ],
      "source": [
        "ROOT_DIR = \".\"\n",
        "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
        "SAVE_DIR = os.path.join(ROOT_DIR, \"csv\")\n",
        "RANDOM_STATE = 110\n",
        "\n",
        "train_data = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
        "print(train_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvupT0S-uqlc"
      },
      "source": [
        "### data 처리 v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3aRn4beuqlc"
      },
      "outputs": [],
      "source": [
        "lst3 = ['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam',\n",
        "       'CURE END POSITION X Collect Result_Dam',\n",
        "       'CURE SPEED Collect Result_Dam',\n",
        "\n",
        "       'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage1) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage2) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage3) Collect Result_Dam',\n",
        "\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam',\n",
        "\n",
        "       'HEAD Standby Position X Collect Result_Dam',\n",
        "       'Production Qty Collect Result_Dam', 'Receip No Collect Result_Dam',\n",
        "       'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
        "       'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam',\n",
        "       '1st Pressure Collect Result_AutoClave',\n",
        "       '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
        "       '2nd Pressure Collect Result_AutoClave',\n",
        "       '3rd Pressure Collect Result_AutoClave',\n",
        "       '3rd Pressure Unit Time_AutoClave',\n",
        "       'Chamber Temp. Collect Result_AutoClave',\n",
        "       'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1',\n",
        "\n",
        "       'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage1) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage2) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage3) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
        "       'Head Purge Position Z Collect Result_Fill1', 'Equipment_Fill2',\n",
        "       'CURE END POSITION X Collect Result_Fill2',\n",
        "       'CURE END POSITION Z Collect Result_Fill2',\n",
        "       'CURE SPEED Collect Result_Fill2',\n",
        "       'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
        "       'WorkMode Collect Result_Fill2',\n",
        "        'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj2sO7-luqlc",
        "outputId": "4381baa3-8fc4-4e50-fa9f-ba2c4ed15187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40506, 61)\n"
          ]
        }
      ],
      "source": [
        "train_data_v1 = train_data[lst3]\n",
        "print(train_data_v1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7VPvQGouqld",
        "outputId": "d3323b8c-c2fc-41e0-bdaa-4c8b7c707042"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_66025/1499500245.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "# Normal: 0, AbNormal: 1\n",
        "train_data_v1['target'] = train_data_v1['target'].apply(lambda x: 0 if x == 'Normal' else 1)\n",
        "# train_data_v1.to_csv(os.path.join(SAVE_DIR, \"data_prep_1.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izDwp18ruqld",
        "outputId": "734adc31-e381-4fca-d50d-e82c6902678f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_66025/1439439532.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "# OK랑 숫자 섞인 column 에서 ok -> null로 변경\n",
        "outlier_ok = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "              'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2']\n",
        "train_data_v1[outlier_ok] = train_data_v1[outlier_ok].apply(pd.to_numeric, errors='coerce').fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIy4pYaMuqld",
        "outputId": "9da18aaa-52cb-4400-c8dc-86e92528e136"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_66025/3717964110.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/tmp/ipykernel_66025/3717964110.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/tmp/ipykernel_66025/3717964110.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "# Equipment_xxx 값 #1, #2로 통일\n",
        "train_data_v1['Equipment_Dam'] = train_data_v1['Equipment_Dam'].replace({\n",
        "    'Dam dispenser #1': '#1',\n",
        "    'Dam dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "train_data_v1['Equipment_Fill1'] = train_data_v1['Equipment_Fill1'].replace({\n",
        "    'Fill1 dispenser #1': '#1',\n",
        "    'Fill1 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "train_data_v1['Equipment_Fill2'] = train_data_v1['Equipment_Fill2'].replace({\n",
        "    'Fill2 dispenser #1': '#1',\n",
        "    'Fill2 dispenser #2': '#2'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKiMqxC-uqld"
      },
      "outputs": [],
      "source": [
        "# 모든 열에서 값을 숫자로 변환 시도\n",
        "train_data_v1 = train_data_v1.apply(pd.to_numeric, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-DHz2Jhuqld"
      },
      "outputs": [],
      "source": [
        "columns_of_interest = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
        "       'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'\n",
        "      ]\n",
        "\n",
        "modified_intervals_list = [\n",
        "[(152.4, 172.4), (538.5, 560.3)],\n",
        "[(151.2, 174.5), (452.2, 475.5), (538.9, 562.1)],\n",
        "[(149.5, 170.8), (452.2, 477.5), (541.1, 562.0)],\n",
        "[(149.8, 172.9), (367.3, 387.5), (540.5, 564.0), (1259.0, 1281.8)],\n",
        "[(367.0, 394.0), (1258.2, 1291.8)],\n",
        "[(367.1, 393.1), (1259.0, 1283.2), (1373.9, 1404.2)],\n",
        "[(272.15, 294.8), (367.3, 394.0), (1261.8, 1292.0)],\n",
        "[(271.8, 276.78), (278.894, 286.8)],\n",
        "[(271.8, 276.78), (278.894, 286.8)],\n",
        "[(827.5, 848.4)],\n",
        "[(146.1, 167.0), (448.2, 470.5), (827.4, 848.4)],\n",
        "[(147.0, 168.0), (447.6, 468.8), (828.1, 848.4)],\n",
        "[(146.1, 167.0), (420.2, 440.5), (828.1, 848.4), (1313.2, 1333.2)],\n",
        "[(419.8, 441.1), (1312.5, 1342.8)],\n",
        "[(419.5, 440.8), (1312.5, 1335.8)],\n",
        "[(234.2, 254.6), (420.0, 441.1), (1312.8, 1342.7)],\n",
        "[(234.2, 254.728)],\n",
        "[(234.2, 254.728)]\n",
        "]\n",
        "\n",
        "\n",
        "def getmean(df, col, intervals):\n",
        "  mean_values = []\n",
        "  for s, e in intervals:\n",
        "    filtered_values = df.loc[(df[col] >= s) & (df[col] <= e), col]\n",
        "    mean_value = filtered_values.mean()\n",
        "    mean_values.append(mean_value)\n",
        "  return mean_values\n",
        "\n",
        "def change_to_dis(df, col, intervals, mean_values):\n",
        "  for (s, e), mean_value in zip(intervals, mean_values):\n",
        "     df.loc[(df[col] >= s) & (df[col] <= e), col] -= mean_value\n",
        "\n",
        "def scale_col(df, columns_of_interest, scale_factor):\n",
        "  df[columns_of_interest] = df[columns_of_interest] * scale_factor\n",
        "\n",
        "def euclidean_dis(df):\n",
        "  for stage in ['Stage1', 'Stage2', 'Stage3']:\n",
        "    # 컬럼 이름 생성\n",
        "    x_col_dam = f'HEAD NORMAL COORDINATE X AXIS({stage}) Collect Result_Dam'\n",
        "    y_col_dam = f'HEAD NORMAL COORDINATE Y AXIS({stage}) Collect Result_Dam'\n",
        "    z_col_dam = f'HEAD NORMAL COORDINATE Z AXIS({stage}) Collect Result_Dam'\n",
        "\n",
        "    x_col_fill = f'HEAD NORMAL COORDINATE X AXIS({stage}) Collect Result_Fill1'\n",
        "    y_col_fill = f'HEAD NORMAL COORDINATE Y AXIS({stage}) Collect Result_Fill1'\n",
        "    z_col_fill = f'HEAD NORMAL COORDINATE Z AXIS({stage}) Collect Result_Fill1'\n",
        "\n",
        "    # 유클리드 거리 계산\n",
        "    df[f'Euclidean Distance Dam ({stage})'] = np.sqrt(df[x_col_dam]**2 + df[y_col_dam]**2 + df[z_col_dam]**2)\n",
        "    df[f'Euclidean Distance Fill1 ({stage})'] = np.sqrt(df[x_col_fill]**2 + df[y_col_fill]**2 + df[z_col_fill]**2)\n",
        "\n",
        "    df.drop([x_col_dam, y_col_dam, z_col_dam, x_col_fill, y_col_fill, z_col_fill], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXWfd_MDuqld"
      },
      "outputs": [],
      "source": [
        "# outlier 제거\n",
        "train_data_v1 = train_data_v1[train_data_v1['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'] != 225.85]\n",
        "\n",
        "# 좌표 distance로 바꾸기\n",
        "for column, intervals in zip(columns_of_interest, modified_intervals_list):\n",
        "  mean_values = getmean(train_data_v1, column, intervals)\n",
        "  change_to_dis(train_data_v1, column, intervals, mean_values)\n",
        "#scaling\n",
        "scale_col(train_data_v1, columns_of_interest, 10)\n",
        "#euclidean dis f로 변경\n",
        "euclidean_dis(train_data_v1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twwgp_fouqld"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "       'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
        "       'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage1) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage2) Collect Result_Dam',\n",
        "       'Dispense Volume(Stage3) Collect Result_Dam',\n",
        "\n",
        "              'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
        "       'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage1) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage2) Collect Result_Fill1',\n",
        "       'Dispense Volume(Stage3) Collect Result_Fill1',\n",
        "'''\n",
        "\n",
        "def make_resin_feature(df):\n",
        "  for i in range(1, 4):\n",
        "    time_dam = f'DISCHARGED TIME OF RESIN(Stage{i}) Collect Result_Dam'\n",
        "    volumn_dam = f'Dispense Volume(Stage{i}) Collect Result_Dam'\n",
        "    time_fill1 = f'DISCHARGED TIME OF RESIN(Stage{i}) Collect Result_Fill1'\n",
        "    volumn_fill1 = f'Dispense Volume(Stage{i}) Collect Result_Fill1'\n",
        "    resin_dam = df['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df[time_dam] / df[volumn_dam]\n",
        "    resin_fill1 = df['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df[time_fill1] * 100 / df[volumn_fill1] - 1100\n",
        "    df[f'new_resin_dam_stage{i}'] = resin_dam\n",
        "    df[f'new_resin_fill1_stage{i}'] = resin_fill1\n",
        "def drop_origin_regin(df):\n",
        "  columns_to_drop = [\n",
        "    'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
        "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
        "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
        "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
        "    'Dispense Volume(Stage1) Collect Result_Dam',\n",
        "    'Dispense Volume(Stage2) Collect Result_Dam',\n",
        "    'Dispense Volume(Stage3) Collect Result_Dam',\n",
        "    'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
        "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
        "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
        "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
        "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
        "    'Dispense Volume(Stage2) Collect Result_Fill1',\n",
        "    'Dispense Volume(Stage3) Collect Result_Fill1']\n",
        "  df.drop(columns=columns_to_drop, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyc38Y1kuqle",
        "outputId": "794b0a93-6267-4633-879a-81c1aa50b74e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nnew_resin_dam_stage1\\nnew_resin_dam_stage2\\nnew_resin_dam_stage3\\nnew_resin_fill1_stage1\\nnew_resin_fill1_stage2\\nnew_resin_fill1_stage3\\n'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "make_resin_feature(train_data_v1)\n",
        "drop_origin_regin(train_data_v1)\n",
        "train_data_v1.to_csv(os.path.join(SAVE_DIR, \"data_prep_resin.csv\"), index=False)\n",
        "'''\n",
        "new_resin_dam_stage1\n",
        "new_resin_dam_stage2\n",
        "new_resin_dam_stage3\n",
        "new_resin_fill1_stage1\n",
        "new_resin_fill1_stage2\n",
        "new_resin_fill1_stage3\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT3O1PAKuqle"
      },
      "source": [
        "## train v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7FjLxaRuqle",
        "outputId": "7445ce73-36dd-4d27-b018-8f5feb0009dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam', 'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2']\n"
          ]
        }
      ],
      "source": [
        "# 모든 열에서 숫자로 변환이 불가능한 열(column) 찾기\n",
        "non_numeric_columns = train_data_v1.columns[train_data_v1.apply(lambda col: pd.to_numeric(col, errors='coerce').notna().all()) == False].tolist()\n",
        "print(non_numeric_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt510zx7uqle"
      },
      "source": [
        "balanced sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq_QwSzDuqle",
        "outputId": "b7aa3339-f7c7-4386-be49-002dd60bf8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({0: 19078, 1: 19078})\n",
            "target\n",
            "0    19078\n",
            "1    19078\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "category = non_numeric_columns\n",
        "X = train_data_v1.drop('target', axis=1)\n",
        "y = train_data_v1['target']\n",
        "\n",
        "# 범주형 변수의 인덱스 목록 생성\n",
        "categorical_features = [X.columns.get_loc(col) for col in category]\n",
        "\n",
        "# SMOTENC로 소수 클래스의 데이터를 다수 클래스의 절반 정도로 증강\n",
        "smote_nc = SMOTENC(categorical_features=categorical_features, sampling_strategy=0.5, random_state=42)\n",
        "\n",
        "# Undersampling을 통해 다수 클래스의 데이터를 소수 클래스와 동일하게 줄이기\n",
        "undersample = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
        "\n",
        "# 파이프라인을 통해 SMOTENC와 Undersampling 결합\n",
        "pipeline = Pipeline(steps=[('smote', smote_nc), ('undersample', undersample)])\n",
        "\n",
        "# 데이터에 파이프라인 적용하여 Resampling 수행\n",
        "X_res, y_res = pipeline.fit_resample(X, y)\n",
        "\n",
        "# Resampling 후 클래스 분포 확인\n",
        "print(f'Resampled dataset shape {Counter(y_res)}')\n",
        "\n",
        "# 결과 확인\n",
        "df_resampled = pd.concat([pd.DataFrame(X_res, columns=X.columns), pd.DataFrame(y_res, columns=['target'])], axis=1)\n",
        "# print(df_resampled)\n",
        "print(df_resampled['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TEbFu9Guqle",
        "outputId": "fbdeebe2-4a30-4f2f-f8f0-e4f192b45ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "bestTest = 0.9346965699\n",
            "bestIteration = 701\n",
            "\n",
            "Shrink model to first 702 iterations.\n",
            "Fold F1 Score: 0.9346965699208444\n",
            "\n",
            "bestTest = 0.9336858339\n",
            "bestIteration = 996\n",
            "\n",
            "Shrink model to first 997 iterations.\n",
            "Fold F1 Score: 0.9336858338841154\n",
            "\n",
            "bestTest = 0.9369646387\n",
            "bestIteration = 906\n",
            "\n",
            "Shrink model to first 907 iterations.\n",
            "Fold F1 Score: 0.9369646386997584\n",
            "\n",
            "bestTest = 0.938579235\n",
            "bestIteration = 796\n",
            "\n",
            "Shrink model to first 797 iterations.\n",
            "Fold F1 Score: 0.9385792349726776\n",
            "Mean F1 Score: 0.935981569369349\n"
          ]
        }
      ],
      "source": [
        "X = df_resampled.drop('target', axis=1)\n",
        "y = df_resampled['target']\n",
        "\n",
        "# 범주형 변수 인덱스 찾기\n",
        "# categorical_features\n",
        "\n",
        "# Stratified K-Fold Cross-Validation 설정\n",
        "n_splits = 4  # K 값 설정\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "# K-Fold Cross-Validation을 통한 모델 학습 및 평가\n",
        "for train_index, val_index in skf.split(X, y):\n",
        "    x_train, x_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.1,\n",
        "        depth=6,\n",
        "        eval_metric='F1',\n",
        "        random_seed=42,\n",
        "        logging_level='Verbose',\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x_train, y_train,\n",
        "        cat_features=categorical_features,\n",
        "        eval_set=(x_val, y_val),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # 예측 및 평가\n",
        "    y_pred = model.predict(x_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    f1_scores.append(f1)\n",
        "    print(f\"Fold F1 Score: {f1}\")\n",
        "\n",
        "# 전체 F1 Score 평균\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "print(f\"Mean F1 Score: {mean_f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1z_6DInuqle"
      },
      "source": [
        "## test data 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW7NylaSuqle"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-6PFLvHuqlf",
        "outputId": "a5e0b2b0-6cb9-4ead-8bc4-9d7536bdb524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17361, 61)\n"
          ]
        }
      ],
      "source": [
        "test_data_v1 = test_data[lst3]\n",
        "print(test_data_v1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R38s_YDiuqlf",
        "outputId": "a4e57419-6df7-45b8-c992-2a09447ca435"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_66025/1958763779.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "# Normal: 0, AbNormal: 1\n",
        "test_data_v1['target'] = test_data_v1['target'].apply(lambda x: 0 if x == 'Normal' else 1)\n",
        "# train_data_v1.to_csv(os.path.join(SAVE_DIR, \"data_prep_1.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a_hnRbcuqlf",
        "outputId": "d277ba51-3b13-4207-e8ba-1dc00bb0966e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_66025/3911089563.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "# OK랑 숫자 섞인 column 에서 ok -> null로 변경\n",
        "outlier_ok = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "              'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2']\n",
        "test_data_v1[outlier_ok] = test_data_v1[outlier_ok].apply(pd.to_numeric, errors='coerce').fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIU4eBcnuqlf",
        "outputId": "639d8435-179e-4731-e418-a14f191a1557"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_66025/4100492348.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/tmp/ipykernel_66025/4100492348.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/tmp/ipykernel_66025/4100492348.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "# Equipment_xxx 값 #1, #2로 통일\n",
        "test_data_v1['Equipment_Dam'] = test_data_v1['Equipment_Dam'].replace({\n",
        "    'Dam dispenser #1': '#1',\n",
        "    'Dam dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data_v1['Equipment_Fill1'] = test_data_v1['Equipment_Fill1'].replace({\n",
        "    'Fill1 dispenser #1': '#1',\n",
        "    'Fill1 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data_v1['Equipment_Fill2'] = test_data_v1['Equipment_Fill2'].replace({\n",
        "    'Fill2 dispenser #1': '#1',\n",
        "    'Fill2 dispenser #2': '#2'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRHE5LEGuqlf"
      },
      "outputs": [],
      "source": [
        "# 모든 열에서 값을 숫자로 변환 시도\n",
        "test_data_v1 = test_data_v1.apply(pd.to_numeric, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ7cjRIHuqlf"
      },
      "outputs": [],
      "source": [
        "# 좌표 distance로 바꾸기\n",
        "for column, intervals in zip(columns_of_interest, modified_intervals_list):\n",
        "  mean_values = getmean(test_data_v1, column, intervals)\n",
        "  change_to_dis(test_data_v1, column, intervals, mean_values)\n",
        "\n",
        "#scaling\n",
        "scale_col(test_data_v1, columns_of_interest, 10)\n",
        "#euclidean dis 로 변경\n",
        "euclidean_dis(test_data_v1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukbc0ofvuqlf"
      },
      "outputs": [],
      "source": [
        "make_resin_feature(test_data_v1)\n",
        "drop_origin_regin(test_data_v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_MuP3q_uqlg"
      },
      "source": [
        "## train v1 predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_zLddJtuqlg",
        "outputId": "fbb939b9-2087-4323-bf1a-a5116489258b"
      },
      "outputs": [
        {
          "ename": "CatBoostError",
          "evalue": "Bad value for num_feature[non_default_doc_idx=0,feature_idx=28]=\"Normal\": Cannot convert 'b'Normal'' to float",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
            "File \u001b[0;32m_catboost.pyx:2309\u001b[0m, in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:1142\u001b[0m, in \u001b[0;36m_catboost._FloatOrNan\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:952\u001b[0m, in \u001b[0;36m_catboost._FloatOrNanFromString\u001b[0;34m()\u001b[0m\n",
            "\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert 'b'Normal'' to float\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_v1\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m      2\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(test_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbNormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;32m      3\u001b[0m test_data_v1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_pred\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:5156\u001b[0m, in \u001b[0;36mCatBoostClassifier.predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n",
            "\u001b[1;32m   5105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, prediction_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m, ntree_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ntree_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, thread_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[1;32m   5106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[1;32m   5107\u001b[0m \u001b[38;5;124;03m    Predict with data.\u001b[39;00m\n",
            "\u001b[1;32m   5108\u001b[0m \n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m   5154\u001b[0m \u001b[38;5;124;03m              with log probability for every class for each object.\u001b[39;00m\n",
            "\u001b[1;32m   5155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[0;32m-> 5156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:2505\u001b[0m, in \u001b[0;36mCatBoost._predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n",
            "\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;32m   2504\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[0;32m-> 2505\u001b[0m data, data_is_single_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_predict_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_method_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   2506\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_prediction_type(prediction_type)\n",
            "\u001b[1;32m   2508\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:2485\u001b[0m, in \u001b[0;36mCatBoost._process_predict_input_data\u001b[0;34m(self, data, parent_method_name, thread_count, label)\u001b[0m\n",
            "\u001b[1;32m   2483\u001b[0m is_single_object \u001b[38;5;241m=\u001b[39m _is_data_single_object(data)\n",
            "\u001b[1;32m   2484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Pool):\n",
            "\u001b[0;32m-> 2485\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m   2486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_single_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m   2487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m   2488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cat_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m   2489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m   2490\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_embedding_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m   2491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthread_count\u001b[49m\n",
            "\u001b[1;32m   2492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, is_single_object\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:793\u001b[0m, in \u001b[0;36mPool.__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n",
            "\u001b[1;32m    787\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_names, PATH_TYPES):\n",
            "\u001b[1;32m    788\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n",
            "\u001b[1;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m    791\u001b[0m             )\n",
            "\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m    794\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    795\u001b[0m \u001b[38;5;28msuper\u001b[39m(Pool, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/catboost/core.py:1425\u001b[0m, in \u001b[0;36mPool._init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n",
            "\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;32m   1424\u001b[0m     feature_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_transform_tags(feature_tags, feature_names)\n",
            "\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m   1426\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:3976\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:4026\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:3842\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:2808\u001b[0m, in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:2350\u001b[0m, in \u001b[0;36m_catboost.create_num_factor_data\u001b[0;34m()\u001b[0m\n",
            "\n",
            "File \u001b[0;32m_catboost.pyx:2311\u001b[0m, in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n",
            "\n",
            "\u001b[0;31mCatBoostError\u001b[0m: Bad value for num_feature[non_default_doc_idx=0,feature_idx=28]=\"Normal\": Cannot convert 'b'Normal'' to float"
          ]
        }
      ],
      "source": [
        "test_pred = model.predict(test_data_v1)\n",
        "test_pred = np.where(test_pred == 1, 'AbNormal', 'Normal')\n",
        "test_data_v1['target'] = test_pred\n",
        "condition = (\n",
        "    (test_data_v1['Equipment_Dam'] != test_data_v1['Equipment_Fill1']) |\n",
        "    (test_data_v1['Equipment_Dam'] != test_data_v1['Equipment_Fill2']) |\n",
        "    (test_data_v1['Equipment_Fill1'] != test_data_v1['Equipment_Fill2'])\n",
        ")\n",
        "test_data_v1.loc[condition, 'target'] = 'AbNormal'\n",
        "test_pred = test_data_v1['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDJODLNkuqlg",
        "outputId": "a8ff7492-f58d-42b2-bd0a-b38c4a35d454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(os.path.join(SAVE_DIR, \"model1_submission.csv\"), index=False)\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model #6"
      ],
      "metadata": {
        "id": "_wMTD6Dyw5gV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-WBmwcquqlg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwUUjH8Wuqlg"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = \".\"\n",
        "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
        "SAVE_DIR = os.path.join(ROOT_DIR, \"csv\")\n",
        "\n",
        "# Load data\n",
        "train_data_origin = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
        "test_data_origin = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
        "df_sub = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag8x-Mnfuqlg"
      },
      "outputs": [],
      "source": [
        "# Drop columns with more than half of the values missing\n",
        "drop_cols = []\n",
        "for column in train_data_origin.columns:\n",
        "    if (train_data_origin[column].isnull().sum()) == len(train_data_origin):\n",
        "        drop_cols.append(column)\n",
        "\n",
        "print(f'null 행: {len(drop_cols)}/{len(train_data_origin.columns)}')\n",
        "train_data = train_data_origin.drop(drop_cols, axis=1).copy(deep=True)\n",
        "train_data[\"target\"] = (train_data[\"target\"] == 'AbNormal').astype(int)\n",
        "\n",
        "test_data = test_data_origin[train_data.columns].copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrE9RfL4uqlg"
      },
      "outputs": [],
      "source": [
        "has_null = train_data[train_data.columns[train_data.isnull().any()]]\n",
        "\n",
        "for has_null_col in has_null.select_dtypes(include=['object']).columns:\n",
        "    train_data[train_data[[has_null_col]] == 'OK'] = np.nan\n",
        "    train_data[has_null_col] = train_data[has_null_col].astype('float')\n",
        "    test_data[test_data[[has_null_col]] == 'OK'] = np.nan\n",
        "    test_data[has_null_col] = test_data[has_null_col].astype('float')\n",
        "\n",
        "\n",
        "for col in train_data[has_null.columns].columns:\n",
        "    print('train:', col, train_data[col].unique())\n",
        "    print('test:', col, test_data[col].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFYGyfypuqlg"
      },
      "outputs": [],
      "source": [
        "nunique_counts = train_data.nunique()\n",
        "drop_cols = nunique_counts[nunique_counts <= 1].index\n",
        "df_train = train_data.drop(drop_cols, axis=1)\n",
        "\n",
        "df_test = test_data.drop(drop_cols, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6Y-CNqfuqlg"
      },
      "outputs": [],
      "source": [
        "has_null = df_train[df_train.columns[df_train.isnull().any()]]\n",
        "\n",
        "for col in df_train[has_null.columns].columns:\n",
        "    df_train[df_train[[col]].isnull()] = 0\n",
        "    df_test[df_test[[col]].isnull()] = 0\n",
        "    print('train:', col, df_train[col].unique())\n",
        "    print('test:', col, df_test[col].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om2QlmL5uqlg"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
        "\n",
        "corr_matrix = train_data.select_dtypes(['float', 'int', 'boolean']).corr().abs()\n",
        "\n",
        "\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.94)]\n",
        "df_train = df_train.drop(columns=to_drop)\n",
        "df_test = df_test.drop(columns=to_drop)\n",
        "del df_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es1amjZxuqlg"
      },
      "outputs": [],
      "source": [
        "\n",
        "golden_features_1 = {\n",
        "    \"new_features\": [\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"CURE SPEED Collect Result_Fill2\",\n",
        "            \"operation\": \"sum\",\n",
        "            \"score\": 0.2121855674\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"3rd Pressure Unit Time_AutoClave\",\n",
        "            \"feature2\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"operation\": \"ratio\",\n",
        "            \"score\": 0.2126641102\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "            \"feature2\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"operation\": \"ratio\",\n",
        "            \"score\": 0.2127302559\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"3rd Pressure Unit Time_AutoClave\",\n",
        "            \"operation\": \"ratio\",\n",
        "            \"score\": 0.2129125475\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "            \"operation\": \"multiply\",\n",
        "            \"score\": 0.2129371268\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"CURE SPEED Collect Result_Fill2\",\n",
        "            \"operation\": \"multiply\",\n",
        "            \"score\": 0.2129570359\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "            \"operation\": \"ratio\",\n",
        "            \"score\": 0.2129659371\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"Dispense Volume(Stage2) Collect Result_Dam\",\n",
        "            \"operation\": \"sum\",\n",
        "            \"score\": 0.2129880768\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"3rd Pressure Unit Time_AutoClave\",\n",
        "            \"operation\": \"multiply\",\n",
        "            \"score\": 0.2130466599\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"Stage2 Circle1 Distance Speed Collect Result_Dam\",\n",
        "            \"feature2\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"operation\": \"diff\",\n",
        "            \"score\": 0.2131566118\n",
        "        }\n",
        "    ],\n",
        "    \"new_columns\": [\n",
        "        \"WorkMode Collect Result_Fill2_sum_CURE SPEED Collect Result_Fill2\",\n",
        "        \"3rd Pressure Unit Time_AutoClave_ratio_WorkMode Collect Result_Fill2\",\n",
        "        \"1st Pressure 1st Pressure Unit Time_AutoClave_ratio_WorkMode Collect Result_Fill2\",\n",
        "        \"WorkMode Collect Result_Fill2_ratio_3rd Pressure Unit Time_AutoClave\",\n",
        "        \"WorkMode Collect Result_Fill2_multiply_1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "        \"WorkMode Collect Result_Fill2_multiply_CURE SPEED Collect Result_Fill2\",\n",
        "        \"WorkMode Collect Result_Fill2_ratio_1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "        \"WorkMode Collect Result_Fill2_sum_Dispense Volume(Stage2) Collect Result_Dam\",\n",
        "        \"WorkMode Collect Result_Fill2_multiply_3rd Pressure Unit Time_AutoClave\",\n",
        "        \"Stage2 Circle1 Distance Speed Collect Result_Dam_diff_WorkMode Collect Result_Fill2\"\n",
        "    ],\n",
        "    \"ml_task\": \"binary_classification\"\n",
        "}\n",
        "golden_features_2 = {\n",
        "    \"new_features\": [\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"3rd Pressure Unit Time_AutoClave\",\n",
        "            \"operation\": \"sum\",\n",
        "            \"score\": 0.2104471591\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "            \"operation\": \"sum\",\n",
        "            \"score\": 0.210717613\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"Dispense Volume(Stage1) Collect Result_Dam\",\n",
        "            \"operation\": \"sum\",\n",
        "            \"score\": 0.2111184784\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"CURE SPEED Collect Result_Fill2\",\n",
        "            \"operation\": \"sum\",\n",
        "            \"score\": 0.2111915385\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"CURE END POSITION Z Collect Result_Fill2\",\n",
        "            \"operation\": \"sum\",\n",
        "            \"score\": 0.2112053145\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"Dispense Volume(Stage1) Collect Result_Dam\",\n",
        "            \"operation\": \"multiply\",\n",
        "            \"score\": 0.21127006\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"HEAD Standby Position X Collect Result_Dam\",\n",
        "            \"feature2\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"operation\": \"ratio\",\n",
        "            \"score\": 0.211271404\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam\",\n",
        "            \"feature2\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"operation\": \"ratio\",\n",
        "            \"score\": 0.211271404\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"Stage2 Circle1 Distance Speed Collect Result_Dam\",\n",
        "            \"feature2\": \"HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam\",\n",
        "            \"operation\": \"multiply\",\n",
        "            \"score\": 0.2112727579\n",
        "        },\n",
        "        {\n",
        "            \"feature1\": \"WorkMode Collect Result_Fill2\",\n",
        "            \"feature2\": \"HEAD Standby Position X Collect Result_Dam\",\n",
        "            \"operation\": \"multiply\",\n",
        "            \"score\": 0.2112793565\n",
        "        }\n",
        "    ],\n",
        "    \"new_columns\": [\n",
        "        \"WorkMode Collect Result_Fill2_sum_3rd Pressure Unit Time_AutoClave\",\n",
        "        \"WorkMode Collect Result_Fill2_sum_1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "        \"WorkMode Collect Result_Fill2_sum_Dispense Volume(Stage1) Collect Result_Dam\",\n",
        "        \"WorkMode Collect Result_Fill2_sum_CURE SPEED Collect Result_Fill2\",\n",
        "        \"WorkMode Collect Result_Fill2_sum_CURE END POSITION Z Collect Result_Fill2\",\n",
        "        \"WorkMode Collect Result_Fill2_multiply_Dispense Volume(Stage1) Collect Result_Dam\",\n",
        "        \"HEAD Standby Position X Collect Result_Dam_ratio_WorkMode Collect Result_Fill2\",\n",
        "        \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam_ratio_WorkMode Collect Result_Fill2\",\n",
        "        \"Stage2 Circle1 Distance Speed Collect Result_Dam_multiply_HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam\",\n",
        "        \"WorkMode Collect Result_Fill2_multiply_HEAD Standby Position X Collect Result_Dam\"\n",
        "    ],\n",
        "    \"ml_task\": \"binary_classification\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTzys0I5uqlh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('AutoML_3/golden_features.json', 'rt') as f:\n",
        "    golden_features_1 = json.loads(f.read())\n",
        "\n",
        "with open('AutoML_6/golden_features.json', 'rt') as f:\n",
        "    golden_features_2 = json.loads(f.read())\n",
        "\n",
        "common_golden_features = []\n",
        "for new_feature_1 in golden_features_1['new_features']:\n",
        "    for new_feature_2 in golden_features_2['new_features']:\n",
        "        if new_feature_1['operation'] == new_feature_2['operation']:\n",
        "            if (new_feature_1['feature1'] == new_feature_2['feature1'] and new_feature_1['feature2'] == new_feature_2['feature2']) or (new_feature_1['feature1'] == new_feature_2['feature2'] and new_feature_1['feature2'] == new_feature_2['feature1']):\n",
        "                common_golden_features.append(new_feature_1)\n",
        "union_golden_features = golden_features_1['new_features'] + golden_features_2['new_features']\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX9R7YXpuqlh"
      },
      "outputs": [],
      "source": [
        "def make_new_features(df: pd.DataFrame, new_features: dict):\n",
        "    for new_feature in new_features:\n",
        "        new_col = \"_\".join(\n",
        "            [\n",
        "                new_feature[\"feature1\"],\n",
        "                new_feature[\"operation\"],\n",
        "                new_feature[\"feature2\"],\n",
        "            ]\n",
        "        )\n",
        "        if new_feature[\"operation\"] == \"diff\":\n",
        "            df[new_col] = df[new_feature[\"feature1\"]] - df[new_feature[\"feature2\"]]\n",
        "        elif new_feature[\"operation\"] == \"ratio\":\n",
        "            a, b = (\n",
        "                np.array(df[new_feature[\"feature1\"]], dtype=float),\n",
        "                np.array(df[new_feature[\"feature2\"]], dtype=float),\n",
        "            )\n",
        "            df[new_col] = np.divide(\n",
        "                a, b, out=np.zeros_like(a), where=b != 0\n",
        "            ).reshape(-1, 1)\n",
        "        elif new_feature[\"operation\"] == \"sum\":\n",
        "            df[new_col] = df[new_feature[\"feature1\"]] + df[new_feature[\"feature2\"]]\n",
        "        elif new_feature[\"operation\"] == \"multiply\":\n",
        "            df[new_col] = df[new_feature[\"feature1\"]] * df[new_feature[\"feature2\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnc2KDSSuqlh"
      },
      "outputs": [],
      "source": [
        "make_new_features(df_train, common_golden_features)\n",
        "make_new_features(df_test, common_golden_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AWR3ISyuqlh"
      },
      "outputs": [],
      "source": [
        "drop_features_1 = [\n",
        "    \"Workorder_AutoClave\",\n",
        "    \"Dispense Volume(Stage1) Collect Result_Fill1\",\n",
        "    \"Chamber Temp. Collect Result_AutoClave\",\n",
        "    \"Dispense Volume(Stage2) Collect Result_Dam\",\n",
        "    \"WorkMode Collect Result_Fill2_sum_CURE SPEED Collect Result_Fill2\",\n",
        "    \"CURE SPEED Collect Result_Fill2\",\n",
        "    \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1\",\n",
        "    \"Workorder_Fill2\",\n",
        "    \"WorkMode Collect Result_Fill2_multiply_1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "    \"Dispense Volume(Stage1) Collect Result_Dam\",\n",
        "    \"THICKNESS 3 Collect Result_Dam\",\n",
        "    \"1st Pressure 1st Pressure Unit Time_AutoClave_ratio_WorkMode Collect Result_Fill2\",\n",
        "    \"HEAD Standby Position X Collect Result_Dam\",\n",
        "    \"3rd Pressure Unit Time_AutoClave\",\n",
        "    \"WorkMode Collect Result_Fill2_ratio_1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "    \"THICKNESS 2 Collect Result_Dam\",\n",
        "    \"DISCHARGED SPEED OF RESIN Collect Result_Dam\",\n",
        "    \"DISCHARGED SPEED OF RESIN Collect Result_Fill1\",\n",
        "    \"WorkMode Collect Result_Fill2_multiply_3rd Pressure Unit Time_AutoClave\",\n",
        "    \"WorkMode Collect Result_Fill2_multiply_CURE SPEED Collect Result_Fill2\",\n",
        "    \"CURE END POSITION X Collect Result_Fill2\",\n",
        "    \"CURE END POSITION Z Collect Result_Fill2\",\n",
        "    \"WorkMode Collect Result_Fill2\",\n",
        "    \"3rd Pressure Unit Time_AutoClave_ratio_WorkMode Collect Result_Fill2\",\n",
        "    \"Model.Suffix_Fill1\",\n",
        "    \"Model.Suffix_AutoClave\",\n",
        "    \"Model.Suffix_Fill2\",\n",
        "    \"WorkMode Collect Result_Fill2_ratio_3rd Pressure Unit Time_AutoClave\",\n",
        "    \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\",\n",
        "    \"Stage2 Circle1 Distance Speed Collect Result_Dam\",\n",
        "    \"DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1\",\n",
        "    \"CURE SPEED Collect Result_Dam\",\n",
        "    \"Head Purge Position Z Collect Result_Fill1\",\n",
        "]\n",
        "\n",
        "drop_features_2 = [\n",
        "    \"1st Pressure 1st Pressure Unit Time_AutoClave\",\n",
        "    \"Chamber Temp. Collect Result_AutoClave\",\n",
        "    \"Stage2 Circle1 Distance Speed Collect Result_Dam\",\n",
        "    \"CURE SPEED Collect Result_Dam\",\n",
        "    \"CURE END POSITION X Collect Result_Fill2\",\n",
        "    \"DISCHARGED SPEED OF RESIN Collect Result_Dam\",\n",
        "    \"THICKNESS 3 Collect Result_Dam\",\n",
        "    \"HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPRzjp4Uuqlh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "comm_drop_features = [feat for feat in drop_features_1 if feat in drop_features_2]\n",
        "\n",
        "for feat in comm_drop_features.copy():\n",
        "    if feat not in df_train.columns:\n",
        "        comm_drop_features.remove(feat)\n",
        "\n",
        "union_drop_features = list(set(drop_features_1 + drop_features_2))\n",
        "for feat in union_drop_features.copy():\n",
        "    if feat not in df_train.columns:\n",
        "        union_drop_features.remove(feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NA7cdxluqlh"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop(columns=union_drop_features)\n",
        "df_test = df_test.drop(columns=union_drop_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB7GxVVmuqlh"
      },
      "outputs": [],
      "source": [
        "class VotingModel():\n",
        "    def __init__(self, estimators):\n",
        "        self.estimators = estimators\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
        "        return (np.mean(y_preds, axis=0) >= 0.5).astype(int)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
        "        return np.mean(y_preds, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFvJ3H3nuqlh"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer, MaxAbsScaler\n",
        "\n",
        "y = df_train['target']\n",
        "X = df_train.drop('target', axis=1)\n",
        "object_cols = X.select_dtypes(include='object').columns\n",
        "num_cols = X.select_dtypes(include=['int', 'float']).columns\n",
        "\n",
        "numeric_transformer = MaxAbsScaler()\n",
        "categorical_transformer = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
        "ct = ColumnTransformer([\n",
        "    ('num', numeric_transformer, num_cols),\n",
        "    ('cat', categorical_transformer, object_cols)])\n",
        "ct.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMfftD69uqlh"
      },
      "outputs": [],
      "source": [
        "class VotingModel():\n",
        "    def __init__(self, estimators):\n",
        "        self.estimators = estimators\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
        "        return (np.mean(y_preds, axis=0) >= 0.5).astype(int)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
        "        return np.mean(y_preds, axis=0)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"model6.pkl\"), \"rb\") as f:\n",
        "    model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsPhO2Vnuqlh"
      },
      "outputs": [],
      "source": [
        "X_test = df_test.drop(columns=['target'])\n",
        "X_test = ct.transform(X_test)\n",
        "pred_y = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x83VL8Ouqlh"
      },
      "outputs": [],
      "source": [
        "df_sub['target'] = pred_y\n",
        "df_sub[df_sub==1] = 'AbNormal'\n",
        "df_sub[df_sub==0] = 'Normal'\n",
        "df_sub.to_csv(os.path.join(SAVE_DIR, 'model6_submission.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Ensemble"
      ],
      "metadata": {
        "id": "TOYaFPEXxSCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jifPJs0uqlh"
      },
      "outputs": [],
      "source": [
        "df_model1 = pd.read_csv(os.path.join(SAVE_DIR, \"model1_submission.csv\"))\n",
        "df_model2 = pd.read_csv(os.path.join(SAVE_DIR, \"model2_submission.csv\"))\n",
        "df_model5 = pd.read_csv(os.path.join(SAVE_DIR, \"model5_submission.csv\"))\n",
        "df_model6 = pd.read_csv(os.path.join(SAVE_DIR, \"model6_submission.csv\"))\n",
        "\n",
        "df_model1[\"target\"] = (df_model1[\"target\"] == 'AbNormal').astype(int)\n",
        "df_model2[\"target\"] = (df_model2[\"target\"] == 'AbNormal').astype(int)\n",
        "df_model5[\"target\"] = (df_model5[\"target\"] == 'AbNormal').astype(int)\n",
        "df_model6[\"target\"] = (df_model6[\"target\"] == 'AbNormal').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z7kw9suuqli"
      },
      "outputs": [],
      "source": [
        "df_list = [df_model1, df_model2, df_model5, df_model6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIER-uXtuqli"
      },
      "outputs": [],
      "source": [
        "def apply_rules(df):\n",
        "    receip_columns = [col for col in df.columns if 'Receip No Collect Result' in col]\n",
        "    equipment_columns = [col for col in df.columns if 'Equipment' in col and col != 'Equipment_AutoClave']\n",
        "    qty_columns = [col for col in df.columns if 'Production Qty Collect Result' in col]\n",
        "\n",
        "    condition = (\n",
        "        (df[receip_columns].nunique(axis=1) > 1) |\n",
        "        (df[equipment_columns].nunique(axis=1) > 1) |\n",
        "        (df[qty_columns].nunique(axis=1) > 1) |\n",
        "        (df['Workorder_Dam'].isin([\"3KPXX094-0001\", \"4CPXX084-0001\"]))\n",
        "    )\n",
        "\n",
        "    return condition\n",
        "\n",
        "test_data ['Equipment_Dam'] = test_data ['Equipment_Dam'].replace({\n",
        "    'Dam dispenser #1': '#1',\n",
        "    'Dam dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data ['Equipment_Fill1'] = test_data ['Equipment_Fill1'].replace({\n",
        "    'Fill1 dispenser #1': '#1',\n",
        "    'Fill1 dispenser #2': '#2'\n",
        "})\n",
        "\n",
        "test_data ['Equipment_Fill2'] = test_data ['Equipment_Fill2'].replace({\n",
        "    'Fill2 dispenser #1': '#1',\n",
        "    'Fill2 dispenser #2': '#2'\n",
        "})\n",
        "condition = apply_rules(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGN9c1B_uqli"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_sub['target'] = 0\n",
        "for i, df in enumerate(df_list):\n",
        "    df_sub['target'] += df['target']\n",
        "df_sub['target'] = (df_sub['target'] / len(df_list)) >= 0.5\n",
        "df_sub['target'] = df_sub['target'].astype('int')\n",
        "df_sub.loc[condition, 'target']=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNMavSOCuqli"
      },
      "outputs": [],
      "source": [
        "df_sub[df_sub==1] = 'AbNormal'\n",
        "df_sub[df_sub==0] = 'Normal'\n",
        "df_sub.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}